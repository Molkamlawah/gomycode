# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1abp5f1-FD-LTvGzYOnBMTFwNSATrx8s4
"""

import requests
from bs4 import BeautifulSoup

# Get and parse html content from a Wikipedia page
def get_html_content(url):
    response = requests.get(url)
    return response.content

# Extract article title
def extract_article_title(soup):
    return soup.find('title').get_text()

# Extract article text for each paragraph with headings and map them
def extract_article_text(soup):
    article_dict = {}
    current_heading = None
    paragraphs = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
    for element in paragraphs:
        if element.name.startswith('h'):
            current_heading = element.get_text()
            article_dict[current_heading] = ""
        elif current_heading:
            article_dict[current_heading] += element.get_text()
    return article_dict

# Collect every link that redirects to another Wikipedia page
def extract_wikipedia_links(soup):
    return [link['href'] for link in soup.find_all('a', href=True) if link['href'].startswith('/wiki/')]

# Wrap all functions into a single function
def scrape_wikipedia_page(url):
    html_content = get_html_content(url)
    soup = BeautifulSoup(html_content, 'html.parser')
    return {
        'title': extract_article_title(soup),
        'article_text': extract_article_text(soup),
        'wikipedia_links': extract_wikipedia_links(soup)
    }

# Test the function on a Wikipedia page of your choice
test_link = "https://en.wikipedia.org/wiki/Artificial_intelligence"
result = scrape_wikipedia_page(test_link)
print(result)